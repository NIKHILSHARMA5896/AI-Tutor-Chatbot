{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded from environment variables.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    \n",
    "    # Make sure to replace 'OPENAI_API_KEY1' with the actual name you gave your secret key in Google Colab secrets. \n",
    "    api_key = os.getenv('OPENAI_API_KEY')\n",
    "    if api_key:\n",
    "        print(\"API key loaded from environment variables.\")\n",
    "    \n",
    "    if not api_key:\n",
    "        print(\"OpenAI API key not found in environment variables.\")\n",
    "        api_key = input(\"Please enter your OpenAI API key manually: \")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Not running in local environment.\")\n",
    "    api_key = input(\"Please enter your OpenAI API key manually: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key loaded successfully (starting with: sk-p...).\n"
     ]
    }
   ],
   "source": [
    "# Final validation\n",
    "if not api_key:\n",
    "    raise ValueError(\"API Key not provided. Please ensure it's set.\")\n",
    "else:\n",
    "    print(f\"API Key loaded successfully (starting with: {api_key[:4]}...).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Initialize the OpenAI Client ---\n",
    "# This creates an OpenAI client object using your API key.\n",
    "# All subsequent API calls will be made through this 'client' object.\n",
    "try:\n",
    "    client = openai.OpenAI(api_key=api_key)\n",
    "    print(\"OpenAI client initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing OpenAI client: {e}\")\n",
    "    # You might want to exit or raise the error here depending on desired behavior\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Conversation: Turn 1 ---\n",
      "Sending messages: [{'role': 'system', 'content': 'You are a helpful AI Tutor explaining Large Language Model concepts simply.'}, {'role': 'user', 'content': \"Can you explain what 'tokens' are in the context of LLMs, like I'm new to this?\"}]\n"
     ]
    }
   ],
   "source": [
    "# --- Block 2: First Turn - Asking the Initial Question ---\n",
    "print(\"\\n--- Starting Conversation: Turn 1 ---\")\n",
    "\n",
    "# Define the system message (persona) for the AI Tutor\n",
    "system_message = {\"role\": \"system\", \"content\": \"You are a helpful AI Tutor explaining Large Language Model concepts simply.\"}\n",
    "\n",
    "# Define the user's first question\n",
    "user_message_1 = {\"role\": \"user\", \"content\": \"Can you explain what 'tokens' are in the context of LLMs, like I'm new to this?\"}\n",
    "\n",
    "# Create the messages list for the *first* API call\n",
    "messages_history = [\n",
    "    system_message,\n",
    "    user_message_1\n",
    "]\n",
    "\n",
    "print(f\"Sending messages: {messages_history}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for this call\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "TEMPERATURE = 0.5 # Slightly creative but still grounded explanation\n",
    "MAX_TOKENS = 150  # Limit the length of the explanation\n",
    "SEED = 123        # Make this explanation reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Making API call to gpt-4o-mini...\n",
      "API call successful.\n",
      "\n",
      "AI Tutor (Turn 1):\n",
      "Sure! In the context of Large Language Models (LLMs), a \"token\" is a basic unit of text that the model processes. Tokens can be words, parts of words, or even punctuation marks. \n",
      "\n",
      "Think of it this way: when you write a sentence, it’s made up of words. However, for an LLM, it breaks down that sentence into smaller pieces called tokens. For example, the sentence “I love cats!” might be broken down into the tokens: “I”, “love”, “cats”, and “!”.\n",
      "\n",
      "Here are a few key points to help you understand tokens better:\n",
      "\n",
      "1. **Granularity**: Tokens can vary in size. Some models use whole words as tokens, while others might use sub\n",
      "\n",
      "Token Usage (Turn 1): Prompt=46, Completion=150, Total=196\n",
      "Finish Reason: length\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(f\"\\nMaking API call to {MODEL}...\")\n",
    "    # Use the client object's method to create a chat completion\n",
    "    completion_1 = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages_history,\n",
    "        temperature=TEMPERATURE,\n",
    "        max_tokens=MAX_TOKENS,\n",
    "        seed=SEED\n",
    "    )\n",
    "    print(\"API call successful.\")\n",
    "\n",
    "    # --- Process the response from the first turn ---\n",
    "    # Extract the assistant's reply content\n",
    "    assistant_response_1 = completion_1.choices[0].message.content\n",
    "    # Extract the full message object to add to history later\n",
    "    assistant_message_1 = completion_1.choices[0].message\n",
    "\n",
    "    print(\"\\nAI Tutor (Turn 1):\")\n",
    "    print(assistant_response_1)\n",
    "\n",
    "    # Print token usage for this call\n",
    "    usage_1 = completion_1.usage\n",
    "    print(f\"\\nToken Usage (Turn 1): Prompt={usage_1.prompt_tokens}, Completion={usage_1.completion_tokens}, Total={usage_1.total_tokens}\")\n",
    "    finish_reason_1 = completion_1.choices[0].finish_reason\n",
    "    print(f\"Finish Reason: {finish_reason_1}\")\n",
    "\n",
    "except openai.APIError as e:\n",
    "    # Handle API errors (e.g., server issues, rate limits)\n",
    "    print(f\"OpenAI API returned an API Error: {e}\")\n",
    "except openai.AuthenticationError as e:\n",
    "    # Handle Authentication errors (e.g., invalid API key)\n",
    "    print(f\"OpenAI Authentication Error: {e}\")\n",
    "except Exception as e:\n",
    "    # Handle other potential errors\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content='Sure! In the context of Large Language Models (LLMs), a \"token\" is a basic unit of text that the model processes. Tokens can be words, parts of words, or even punctuation marks. \\n\\nThink of it this way: when you write a sentence, it’s made up of words. However, for an LLM, it breaks down that sentence into smaller pieces called tokens. For example, the sentence “I love cats!” might be broken down into the tokens: “I”, “love”, “cats”, and “!”.\\n\\nHere are a few key points to help you understand tokens better:\\n\\n1. **Granularity**: Tokens can vary in size. Some models use whole words as tokens, while others might use sub', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, annotations=[])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistant_message_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Continuing Conversation: Turn 2 ---\n",
      "\n",
      "Sending updated messages: [{'role': 'system', 'content': 'You are a helpful AI Tutor explaining Large Language Model concepts simply.'}, {'role': 'user', 'content': \"Can you explain what 'tokens' are in the context of LLMs, like I'm new to this?\"}, ChatCompletionMessage(content='Sure! In the context of Large Language Models (LLMs), a \"token\" is a basic unit of text that the model processes. Tokens can be words, parts of words, or even punctuation marks. \\n\\nThink of it this way: when you write a sentence, it’s made up of words. However, for an LLM, it breaks down that sentence into smaller pieces called tokens. For example, the sentence “I love cats!” might be broken down into the tokens: “I”, “love”, “cats”, and “!”.\\n\\nHere are a few key points to help you understand tokens better:\\n\\n1. **Granularity**: Tokens can vary in size. Some models use whole words as tokens, while others might use sub', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, annotations=[]), {'role': 'user', 'content': \"Thanks! So, based on your explanation, are common words like 'the' or 'is' usually single tokens?\"}]\n"
     ]
    }
   ],
   "source": [
    "# --- Block 3: Second Turn - Asking a Follow-up Question ---\n",
    "print(\"\\n--- Continuing Conversation: Turn 2 ---\")\n",
    "\n",
    "# Assume the first turn was successful and we have 'assistant_message_1'\n",
    "# Define the user's second question, referencing the previous explanation\n",
    "user_message_2 = {\"role\": \"user\", \"content\": \"Thanks! So, based on your explanation, are common words like 'the' or 'is' usually single tokens?\"}\n",
    "\n",
    "# --- CRITICAL STEP: Update the message history ---\n",
    "# Append the assistant's *previous* response to the history\n",
    "messages_history.append(assistant_message_1)\n",
    "# Append the user's *new* question to the history\n",
    "messages_history.append(user_message_2)\n",
    "\n",
    "print(f\"\\nSending updated messages: {messages_history}\") # Notice how the list has grown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the second call (could be the same or different)\n",
    "# Let's make it slightly more deterministic for a factual answer\n",
    "TEMPERATURE_2 = 2.0\n",
    "MAX_TOKENS_2 = 300\n",
    "# Using the same seed ensures the *entire conversation flow* is reproducible if inputs are identical\n",
    "SEED_2 = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Making API call to gpt-4o-mini (Turn 2)...\n",
      "API call successful.\n",
      "\n",
      "AI Tutor (Turn 2):\n",
      "Yes, that's correct! Common words like \"the\" or \"is\" are typically treated as single tokens in most language models. Since these words are frequently used and are short, they are usually represented as individual tokens.\n",
      "\n",
      "However, it's important to note that the way tokens are defined can vary depending on the specific model and its tokenization method. Some models might break down longer or less common words into smaller parts, but common words are generally kept as single tokens. This helps the model understand and generate text more effectively!\n",
      "\n",
      "Token Usage (Turn 2): Prompt=228, Completion=300, Total=528\n",
      "Finish Reason: length\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(f\"\\nMaking API call to {MODEL} (Turn 2)...\")\n",
    "    completion_2 = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages_history, # Send the *full* history\n",
    "        temperature=TEMPERATURE_2,\n",
    "        max_tokens=MAX_TOKENS_2,\n",
    "        seed=SEED_2\n",
    "        #stream=True\n",
    "    )\n",
    "    print(\"API call successful.\")\n",
    "\n",
    "    # --- Process the response from the second turn ---\n",
    "    #assistant_response_2 = completion_2.choices[0].message.content\n",
    "    # We don't strictly need to save assistant_message_2 unless continuing the conversation\n",
    "\n",
    "    print(\"\\nAI Tutor (Turn 2):\")\n",
    "    print(assistant_response_2)\n",
    "\n",
    "    # Print token usage for this call\n",
    "    usage_2 = completion_2.usage\n",
    "    print(f\"\\nToken Usage (Turn 2): Prompt={usage_2.prompt_tokens}, Completion={usage_2.completion_tokens}, Total={usage_2.total_tokens}\")\n",
    "    # Note: prompt_tokens for turn 2 will be larger as it includes the history from turn 1.\n",
    "    finish_reason_2 = completion_2.choices[0].finish_reason\n",
    "    print(f\"Finish Reason: {finish_reason_2}\")\n",
    "\n",
    "except openai.APIError as e:\n",
    "    print(f\"OpenAI API returned an API Error: {e}\")\n",
    "except openai.AuthenticationError as e:\n",
    "    print(f\"OpenAI Authentication Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "150\n",
      "196\n"
     ]
    }
   ],
   "source": [
    "# # Example usage object from completion_1 or completion_2:\n",
    "print(usage_1.prompt_tokens)  # -> number of input tokens\n",
    "print(usage_1.completion_tokens) # -> number of output tokens\n",
    "print(usage_1.total_tokens) # -> sum of both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Block 4: Cost Calculation Function & Example ---\n",
    "\n",
    "def calculate_cost(usage, input_price_per_mil, output_price_per_mil):\n",
    "    \"\"\"Calculates the cost of an API call based on token usage and prices.\n",
    "\n",
    "    Args:\n",
    "        usage: The usage object from the OpenAI completion response\n",
    "               (e.g., completion.usage). It should have attributes\n",
    "               'prompt_tokens' and 'completion_tokens'.\n",
    "        input_price_per_mil: Cost in USD per 1 million input tokens.\n",
    "        output_price_per_mil: Cost in USD per 1 million output tokens.\n",
    "\n",
    "    Returns:\n",
    "        The total cost in USD for the API call, or None if usage is invalid.\n",
    "    \"\"\"\n",
    "    if not usage or not hasattr(usage, 'prompt_tokens') or not hasattr(usage, 'completion_tokens'):\n",
    "        print(\"Warning: Invalid usage object provided for cost calculation.\")\n",
    "        return None\n",
    "\n",
    "    input_cost = (usage.prompt_tokens / 1_000_000) * input_price_per_mil\n",
    "    output_cost = (usage.completion_tokens / 1_000_000) * output_price_per_mil\n",
    "    total_cost = input_cost + output_cost\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cost Calculations (GPT-4o-mini, June 2025 Rates) ---\n",
      "Prices: Input=$0.15/1M, Output=$0.60/1M\n"
     ]
    }
   ],
   "source": [
    "# --- Current Prices (April 2025) for GPT-4o-mini ---\n",
    "PRICE_INPUT_PER_MIL = 0.15\n",
    "PRICE_OUTPUT_PER_MIL = 0.60\n",
    "\n",
    "print(f\"\\n--- Cost Calculations (GPT-4o-mini, June 2025 Rates) ---\")\n",
    "print(f\"Prices: Input=${PRICE_INPUT_PER_MIL:.2f}/1M, Output=${PRICE_OUTPUT_PER_MIL:.2f}/1M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cost for Turn 1:\n",
      "  Prompt Tokens: 46, Completion Tokens: 150\n",
      "  Total Cost: $0.00009690\n",
      "\n",
      "Cost for Turn 2:\n",
      "  Prompt Tokens: 228, Completion Tokens: 300\n",
      "  Total Cost: $0.00021420\n",
      "\n",
      "Total Conversation Cost (Turn 1 + Turn 2): $0.00031110\n"
     ]
    }
   ],
   "source": [
    "# Calculate cost for Turn 1 (assuming completion_1 and usage_1 exist from Block 2)\n",
    "try:\n",
    "    if 'usage_1' in locals(): # Check if usage_1 variable exists\n",
    "         cost_1 = calculate_cost(usage_1, PRICE_INPUT_PER_MIL, PRICE_OUTPUT_PER_MIL)\n",
    "         if cost_1 is not None:\n",
    "              print(f\"\\nCost for Turn 1:\")\n",
    "              print(f\"  Prompt Tokens: {usage_1.prompt_tokens}, Completion Tokens: {usage_1.completion_tokens}\")\n",
    "              print(f\"  Total Cost: ${cost_1:.8f}\")\n",
    "    else:\n",
    "         print(\"\\nSkipping Turn 1 cost calculation (usage_1 not found).\")\n",
    "\n",
    "    # Calculate cost for Turn 2 (assuming completion_2 and usage_2 exist from Block 3)\n",
    "    if 'usage_2' in locals(): # Check if usage_2 variable exists\n",
    "        cost_2 = calculate_cost(usage_2, PRICE_INPUT_PER_MIL, PRICE_OUTPUT_PER_MIL)\n",
    "        if cost_2 is not None:\n",
    "            print(f\"\\nCost for Turn 2:\")\n",
    "            print(f\"  Prompt Tokens: {usage_2.prompt_tokens}, Completion Tokens: {usage_2.completion_tokens}\")\n",
    "            print(f\"  Total Cost: ${cost_2:.8f}\")\n",
    "    else:\n",
    "         print(\"\\nSkipping Turn 2 cost calculation (usage_2 not found).\")\n",
    "\n",
    "    # Calculate total conversation cost\n",
    "    if 'cost_1' in locals() and 'cost_2' in locals() and cost_1 is not None and cost_2 is not None:\n",
    "        total_conversation_cost = cost_1 + cost_2\n",
    "        print(f\"\\nTotal Conversation Cost (Turn 1 + Turn 2): ${total_conversation_cost:.8f}\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"\\nCould not calculate costs, a required variable is missing: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during cost calculation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
